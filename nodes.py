# ========================================
# PythonèŠ‚ç‚¹ä»£ç  (screen_mapper_nodes.py)
# ========================================

import torch
import numpy as np
import cv2
from PIL import Image
import base64
import io
import json
from torchvision import transforms

# ========================================
# èŠ‚ç‚¹1: Canvaså››ç‚¹é€‰æ‹©å™¨ (åªè´Ÿè´£é€‰ç‚¹)
# ========================================

class CanvasFourPointSelector:
    """
    ComfyUIèŠ‚ç‚¹ï¼šä½¿ç”¨Canvasäº¤äº’å¼é€‰æ‹©å››ä¸ªè§’ç‚¹
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "reference_image": ("IMAGE",),  # å‚è€ƒå›¾åƒï¼Œç”¨äºCanvasæ˜¾ç¤º
                "points_store": ("STRING", {"multiline": False, "default": "[]"}),
                "coordinates": ("STRING", {"multiline": False, "default": "[]"}),
                "width": ("INT", {"default": 512, "min": 8, "max": 4096, "step": 8}),
                "height": ("INT", {"default": 512, "min": 8, "max": 4096, "step": 8}),
            },
            "optional": {
                "normalize": ("BOOLEAN", {"default": False}),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "MASK", "IMAGE")
    RETURN_NAMES = ("four_points", "point_info", "selection_mask", "reference_image")
    FUNCTION = "select_four_points"
    CATEGORY = "KJNodes/experimental"
    DESCRIPTION = """
# Canvaså››ç‚¹é€‰æ‹©å™¨

**ä¸“é—¨ç”¨äºåœ¨å›¾åƒä¸Šé€‰æ‹©å››ä¸ªè§’ç‚¹ï¼š**

**æ“ä½œæ–¹å¼ï¼š**
- **Shift + ç‚¹å‡»** æ·»åŠ è§’ç‚¹ï¼ˆæœ€å¤š4ä¸ªï¼‰
- **æ‹–æ‹½ç‚¹** è°ƒæ•´ä½ç½®
- **å³é”®ç‚¹å‡»ç‚¹** åˆ é™¤è¯¥ç‚¹
- æ‹–æ‹½å›¾åƒåˆ°èŠ‚ç‚¹èƒŒæ™¯

**ç‚¹å‡»é¡ºåºå»ºè®®ï¼š** å·¦ä¸Š â†’ å³ä¸Š â†’ å³ä¸‹ â†’ å·¦ä¸‹

**è¾“å‡ºå››ä¸ªç‚¹åæ ‡ä¾›åç»­èŠ‚ç‚¹ä½¿ç”¨**
"""

    def parse_coordinates(self, coordinates):
        """è§£æåæ ‡JSONå­—ç¬¦ä¸²"""
        try:
            if not coordinates or coordinates.strip() == "":
                return []
            
            coords_data = json.loads(coordinates)
            processed_coords = []
            
            for coord in coords_data:
                if isinstance(coord, dict):
                    # KJNodesæ ¼å¼: {"x": 100, "y": 200}
                    x = int(round(coord.get('x', 0)))
                    y = int(round(coord.get('y', 0)))
                    processed_coords.append([x, y])
                elif isinstance(coord, (list, tuple)) and len(coord) >= 2:
                    # æ•°ç»„æ ¼å¼: [100, 200]
                    x = int(round(coord[0]))
                    y = int(round(coord[1]))
                    processed_coords.append([x, y])
            
            return processed_coords
        except Exception as e:
            print(f"åæ ‡è§£æé”™è¯¯: {e}")
            return []

    def create_selection_mask(self, points, img_height, img_width):
        """åˆ›å»ºé€‰æ‹©åŒºåŸŸçš„é®ç½©"""
        mask = np.zeros((img_height, img_width), dtype=np.uint8)
        
        if len(points) >= 4:
            # ä½¿ç”¨å‰4ä¸ªç‚¹åˆ›å»ºå¤šè¾¹å½¢é®ç½©
            pts = np.array(points[:4], dtype=np.int32)
            cv2.fillPoly(mask, [pts], 255)
        
        mask_tensor = torch.from_numpy(mask).float().unsqueeze(0) / 255.0
        return mask_tensor

    def select_four_points(self, reference_image, points_store, coordinates, width, height, normalize=False):
        
        # è§£æåæ ‡
        screen_points = self.parse_coordinates(coordinates)
        
        # è·å–å›¾åƒå°ºå¯¸
        if len(reference_image.shape) == 4:
            img_tensor = reference_image[0]
        else:
            img_tensor = reference_image
            
        img_height, img_width = img_tensor.shape[:2]
        
        # å¤„ç†åæ ‡å½’ä¸€åŒ–
        if normalize and screen_points:
            for point in screen_points:
                point[0] = int(point[0] * img_width / width)
                point[1] = int(point[1] * img_height / height)
        
        # é™åˆ¶åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
        for point in screen_points:
            point[0] = max(0, min(point[0], img_width))
            point[1] = max(0, min(point[1], img_height))
        
        # ç”Ÿæˆç‚¹ä¿¡æ¯
        point_labels = ["å·¦ä¸Š", "å³ä¸Š", "å³ä¸‹", "å·¦ä¸‹"]
        point_info_list = []
        
        for i, point in enumerate(screen_points[:4]):
            label = point_labels[i] if i < len(point_labels) else f"ç‚¹{i+1}"
            point_info_list.append(f"{label}: ({point[0]}, {point[1]})")
        
        point_info = " | ".join(point_info_list) if point_info_list else "æœªé€‰æ‹©ä»»ä½•ç‚¹"
        
        # åˆ›å»ºé€‰æ‹©åŒºåŸŸé®ç½©
        selection_mask = self.create_selection_mask(screen_points, img_height, img_width)
        
        # è¾“å‡ºå››ä¸ªç‚¹çš„åæ ‡ï¼ˆJSONæ ¼å¼ï¼‰
        four_points_json = json.dumps(screen_points[:4])
        
        # ç”Ÿæˆç”¨äºCanvasæ˜¾ç¤ºçš„base64å›¾åƒ
        if reference_image is not None:
            transform = transforms.ToPILImage()
            ref_pil = transform(reference_image[0].permute(2, 0, 1))
            buffered = io.BytesIO()
            ref_pil.save(buffered, format="JPEG", quality=85)
            img_bytes = buffered.getvalue()
            img_base64 = base64.b64encode(img_bytes).decode('utf-8')
            
            return {
                "ui": {"bg_image": [img_base64]},
                "result": (four_points_json, point_info, selection_mask, reference_image)
            }
        else:
            return (four_points_json, point_info, selection_mask, reference_image)

# ========================================
# èŠ‚ç‚¹2: é€è§†å˜æ¢æ˜ å°„å™¨ (åªè´Ÿè´£å˜æ¢)
# ========================================

class PerspectiveScreenMapper:
    """
    ComfyUIèŠ‚ç‚¹ï¼šä½¿ç”¨å››ä¸ªç‚¹åæ ‡æ‰§è¡Œé€è§†å˜æ¢æ˜ å°„
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "background_image": ("IMAGE",),  # èƒŒæ™¯å›¾åƒ
                "source_image": ("IMAGE",),      # è¦æ˜ å°„çš„æºå›¾åƒ
                "four_points": ("STRING", {"multiline": False}),  # ä»é€‰æ‹©å™¨èŠ‚ç‚¹æ¥æ”¶çš„å››ä¸ªç‚¹
            },
            "optional": {
                "blend_mode": (["replace", "overlay", "multiply", "screen"], {"default": "replace"}),
                "opacity": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                "crop_to_screen": ("BOOLEAN", {"default": True}),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "IMAGE", "MASK")
    RETURN_NAMES = ("mapped_image", "cropped_screen", "screen_mask")
    FUNCTION = "apply_perspective_mapping"
    CATEGORY = "ImageNodes"
    DESCRIPTION = """
# é€è§†å˜æ¢å±å¹•æ˜ å°„å™¨

**ä½¿ç”¨å››ä¸ªè§’ç‚¹æ‰§è¡Œé€è§†å˜æ¢ï¼š**

**è¾“å…¥ï¼š**
- èƒŒæ™¯å›¾åƒï¼šåŒ…å«å±å¹•çš„å›¾åƒ
- æºå›¾åƒï¼šè¦æ˜ å°„åˆ°å±å¹•çš„å†…å®¹
- å››ä¸ªç‚¹ï¼šä»Canvasé€‰æ‹©å™¨è·å¾—çš„è§’ç‚¹åæ ‡

**è¾“å‡ºï¼š**
- æ˜ å°„åçš„å®Œæ•´å›¾åƒ
- è£å‰ªçš„å±å¹•åŒºåŸŸ
- å±å¹•åŒºåŸŸé®ç½©

**æ··åˆæ¨¡å¼ï¼š** replace, overlay, multiply, screen
"""

    def tensor_to_cv2(self, tensor_image):
        """å°†ComfyUI tensorè½¬æ¢ä¸ºOpenCVæ ¼å¼"""
        if len(tensor_image.shape) == 4:
            tensor_image = tensor_image[0]
        
        np_image = (tensor_image.cpu().numpy() * 255).astype(np.uint8)
        
        if np_image.shape[2] == 3:
            cv2_image = cv2.cvtColor(np_image, cv2.COLOR_RGB2BGR)
        else:
            cv2_image = cv2.cvtColor(np_image, cv2.COLOR_RGBA2BGR)
        
        return cv2_image
    
    def cv2_to_tensor(self, cv2_image):
        """å°†OpenCVæ ¼å¼è½¬æ¢ä¸ºComfyUI tensor"""
        rgb_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)
        tensor_image = torch.from_numpy(rgb_image.astype(np.float32) / 255.0)
        tensor_image = tensor_image.unsqueeze(0)
        return tensor_image

    def parse_four_points(self, four_points_json):
        """è§£æå››ä¸ªç‚¹çš„åæ ‡"""
        try:
            if not four_points_json or four_points_json.strip() == "":
                return []
            
            points = json.loads(four_points_json)
            processed_points = []
            
            for point in points:
                if isinstance(point, dict):
                    x = int(round(point.get('x', 0)))
                    y = int(round(point.get('y', 0)))
                    processed_points.append([x, y])
                elif isinstance(point, (list, tuple)) and len(point) >= 2:
                    x = int(round(point[0]))
                    y = int(round(point[1]))
                    processed_points.append([x, y])
            
            return processed_points[:4]  # åªå–å‰4ä¸ªç‚¹
        except Exception as e:
            print(f"å››ç‚¹åæ ‡è§£æé”™è¯¯: {e}")
            return []

    def get_screen_bbox(self, points):
        """è·å–å±å¹•åŒºåŸŸçš„å¤–æ¥çŸ©å½¢"""
        if len(points) < 4:
            return None
            
        xs = [p[0] for p in points]
        ys = [p[1] for p in points]
        
        x_min = int(min(xs))
        y_min = int(min(ys))
        x_max = int(max(xs))
        y_max = int(max(ys))
        
        return (x_min, y_min, x_max, y_max)

    def create_screen_mask(self, points, img_height, img_width):
        """åˆ›å»ºå±å¹•åŒºåŸŸçš„é®ç½©"""
        mask = np.zeros((img_height, img_width), dtype=np.uint8)
        
        if len(points) >= 4:
            pts = np.array(points[:4], dtype=np.int32)
            cv2.fillPoly(mask, [pts], 255)
        
        mask_tensor = torch.from_numpy(mask).float().unsqueeze(0) / 255.0
        return mask_tensor

    def apply_perspective_transform(self, bg_cv2, src_cv2, dst_points, blend_mode="replace", opacity=1.0):
        """åº”ç”¨é€è§†å˜æ¢æ˜ å°„"""
        bg_height, bg_width = bg_cv2.shape[:2]
        src_height, src_width = src_cv2.shape[:2]
        
        # æºå›¾åƒçš„å››ä¸ªè§’ç‚¹
        src_points = np.array([
            [0, 0],                    # å·¦ä¸Š
            [src_width, 0],            # å³ä¸Š
            [src_width, src_height],   # å³ä¸‹
            [0, src_height]            # å·¦ä¸‹
        ], dtype=np.float32)
        
        # ç›®æ ‡å±å¹•çš„å››ä¸ªç‚¹
        dst_points = np.array(dst_points[:4], dtype=np.float32)
        
        try:
            # è®¡ç®—é€è§†å˜æ¢çŸ©é˜µ
            perspective_matrix = cv2.getPerspectiveTransform(src_points, dst_points)
            
            # åº”ç”¨é€è§†å˜æ¢
            transformed_image = cv2.warpPerspective(
                src_cv2,
                perspective_matrix,
                (bg_width, bg_height),
                flags=cv2.INTER_LINEAR,
                borderMode=cv2.BORDER_TRANSPARENT
            )
            
            # åˆ›å»ºå˜æ¢åŒºåŸŸçš„é®ç½©
            mask = np.zeros((bg_height, bg_width), dtype=np.uint8)
            cv2.fillPoly(mask, [dst_points.astype(int)], 255)
            mask_3ch = cv2.merge([mask, mask, mask]) / 255.0
            
            # æ ¹æ®æ··åˆæ¨¡å¼åˆæˆå›¾åƒ
            if blend_mode == "replace":
                result = bg_cv2.copy()
                result = np.where(mask_3ch > 0, transformed_image, result)
            else:
                blended = self.apply_blend_mode(bg_cv2, transformed_image, blend_mode, opacity)
                result = np.where(mask_3ch > 0, blended, bg_cv2)
            
            return result
            
        except Exception as e:
            print(f"é€è§†å˜æ¢å¤±è´¥: {e}")
            return bg_cv2

    def apply_blend_mode(self, background, foreground, mode, opacity):
        """åº”ç”¨æ··åˆæ¨¡å¼"""
        bg = background.astype(np.float32) / 255.0
        fg = foreground.astype(np.float32) / 255.0
        
        if mode == "overlay":
            mask = fg < 0.5
            result = np.where(mask, 2 * fg * bg, 1 - 2 * (1 - fg) * (1 - bg))
        elif mode == "multiply":
            result = fg * bg
        elif mode == "screen":
            result = 1 - (1 - fg) * (1 - bg)
        else:
            result = fg
        
        # åº”ç”¨é€æ˜åº¦
        result = bg * (1 - opacity) + result * opacity
        return (result * 255).astype(np.uint8)

    def apply_perspective_mapping(self, background_image, source_image, four_points, 
                                blend_mode="replace", opacity=1.0, crop_to_screen=True):
        
        # è§£æå››ä¸ªç‚¹åæ ‡
        screen_points = self.parse_four_points(four_points)
        
        if len(screen_points) < 4:
            print(f"éœ€è¦4ä¸ªç‚¹ï¼Œä½†åªæ”¶åˆ°{len(screen_points)}ä¸ªç‚¹")
            # è¿”å›åŸå§‹å›¾åƒ
            bg_height, bg_width = background_image.shape[1:3]
            empty_mask = torch.zeros((1, bg_height, bg_width), dtype=torch.float32)
            return (background_image, background_image, empty_mask)
        
        # è½¬æ¢å›¾åƒæ ¼å¼
        bg_cv2 = self.tensor_to_cv2(background_image)
        src_cv2 = self.tensor_to_cv2(source_image)
        
        bg_height, bg_width = bg_cv2.shape[:2]
        
        # é™åˆ¶åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
        for point in screen_points:
            point[0] = max(0, min(point[0], bg_width))
            point[1] = max(0, min(point[1], bg_height))
        
        # åº”ç”¨é€è§†å˜æ¢
        result_cv2 = self.apply_perspective_transform(
            bg_cv2, src_cv2, screen_points, blend_mode, opacity
        )
        mapped_image = self.cv2_to_tensor(result_cv2)
        
        # åˆ›å»ºå±å¹•åŒºåŸŸé®ç½©
        screen_mask = self.create_screen_mask(screen_points, bg_height, bg_width)
        
        # è£å‰ªå±å¹•åŒºåŸŸ
        cropped_screen = background_image
        if crop_to_screen:
            bbox = self.get_screen_bbox(screen_points[:4])
            if bbox:
                x_min, y_min, x_max, y_max = bbox
                x_min = max(0, x_min)
                y_min = max(0, y_min)
                x_max = min(bg_width, x_max)
                y_max = min(bg_height, y_max)
                
                if x_max > x_min and y_max > y_min:
                    cropped_screen = mapped_image[:, y_min:y_max, x_min:x_max, :]
        
        return (mapped_image, cropped_screen, screen_mask)

# ========================================
# ComfyUIèŠ‚ç‚¹æ³¨å†Œ
# ========================================

NODE_CLASS_MAPPINGS = {
    "CanvasFourPointSelector": CanvasFourPointSelector,
    "PerspectiveScreenMapper": PerspectiveScreenMapper,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "CanvasFourPointSelector": "ğŸ¯ Canvaså››ç‚¹é€‰æ‹©å™¨",
    "PerspectiveScreenMapper": "ğŸ”„ é€è§†å˜æ¢æ˜ å°„å™¨", 
}